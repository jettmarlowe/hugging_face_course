{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f4d8c5",
   "metadata": {},
   "source": [
    "# What happens inside the pipeline function?\n",
    "* raw text in to tokenizer\n",
    "* input IDs from tokenizer to model\n",
    "* model produces logits\n",
    "* postprocessing creates predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3a5ec5",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "* Text is split into tokens\n",
    "* Special tokens are added if necessary (beginning and end of sentence, etc)\n",
    "* Tokenizer matches each token to its unique ID in the vocabulary of the pre-trained model (maps each token to an integer)\n",
    "* Add anything else that may be helpful here\n",
    "\n",
    "### AutoTokenizer class can load the tokenizer for any checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c37b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_inputs = [\n",
    "    'I have been waiting for a HuggingFace course my whole life.',\n",
    "    'I hate this so much!',\n",
    "]\n",
    "\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# padding=True: Since the two sentences aren't the same size, we need to pad the shortest one.\n",
    "# truncation=True: Any sentence longer than the maximum that the model can handle is truncated.\n",
    "# return_tensors='pt': Return a PyTorch tensor\n",
    "\n",
    "# inputs is a dictionary with two keys ('input_ids' and 'attention_mask')\n",
    "# attention_mask indicates what padding has been applied so the model does not pay attention to it\n",
    "\n",
    "# once you have the tokenizer, you can directly pass your sentences to it \n",
    "# and you will get back a dict that is ready to feed into a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "393759ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1045,  2031,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
       "          2026,  2878,  2166,  1012,   102],\n",
       "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "126ba25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c8b3d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1045,  2031,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
       "         2026,  2878,  2166,  1012,   102])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b6c33",
   "metadata": {},
   "source": [
    "### AutoModel class loads a model without its pretraining head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "431e925f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 15, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf47f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it outputs hidden states, also known as features\n",
    "\n",
    "# 2 is the batch size\n",
    "# 15 is the sequence length\n",
    "# 768 is the hidden size\n",
    "\n",
    "# for each model input, we get a high-dimensional vector representing the \n",
    "# contextual understanding of that input by the Transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ec0dc9",
   "metadata": {},
   "source": [
    "### Model Adaptation Heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbbe96c",
   "metadata": {},
   "source": [
    "The model adaptation heads (also known as model heads) take the high-dimensional vector of hidden states as input and project them onto a different dimension.  \n",
    "They come in different forms and target a specific task, such as language modeling heads, question answering heads, sequence classification heads.  \n",
    "The output of the Transformer model is sent directly to the model head to be processed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b18bb8",
   "metadata": {},
   "source": [
    "### Each AutoModelForXxx class loads a model for a specific task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1666cfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3782,  1.4346],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# we need a model with a sequence classification head (to be able to classify the sentences as positive or negative)\n",
    "\n",
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(outputs.logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df1f32",
   "metadata": {},
   "source": [
    "### To go from logits to probabilities we apply SoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "867bb088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.6637e-02, 9.4336e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d9ed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now they are probabilities that are positive and sum up to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "233950ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the labels\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f9f180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first sentence: NEGATIVE 5.6%, POSITIVE 94.3%\n",
    "# second sentence: NEGATIVE 99.9%, POSITIVE 0.54%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e75233",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e339c01b",
   "metadata": {},
   "source": [
    "#### Instantiate a Transformer Model\n",
    "The AutoModel API allows you to instantiate a pretrained model from any checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c6f03f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab983d484874eca846ebc1ebb8ef380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170e57a1bb2e4867bd5fd9468bd7dbce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_bert.BertModel'>\n",
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2Model'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31409c072f3f486597a1a5f0f87f2cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c6aaba57914c4fa1281bc3c147bf36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bart.modeling_bart.BartModel'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "bert_model = AutoModel.from_pretrained('bert-base-cased')\n",
    "print(type(bert_model))\n",
    "\n",
    "gpt_model = AutoModel.from_pretrained('gpt2')\n",
    "print(type(gpt_model))\n",
    "\n",
    "bart_model = AutoModel.from_pretrained('facebook/bart-base')\n",
    "print(type(bart_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762b89c2",
   "metadata": {},
   "source": [
    "The AutoModel.from_pretrained() gets the checkpoint or local folder and downloads   \n",
    "the config file and instantiates the config class and then gets the model config and loads the model.  \n",
    "The config for a model is a blueprint that contains all the info necessary to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63a0427d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "bert_config = BertConfig.from_pretrained('bert-base-cased')\n",
    "print(bert_config)\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f6ffeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model from the default configuration initializes it with random values\n",
    "# Don't do this\n",
    "\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "config = BertConfig()\n",
    "model = BertModel(config)\n",
    "\n",
    "# Model is randomly initialized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c698a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Instead use the from_pretained method\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad6b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The weights get downloaded and cached to ~/.cache/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b0563",
   "metadata": {},
   "source": [
    "### Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db57fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Able to change any part of the model using keyword arguments\n",
    "# To save a model, use the save_pretrained method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87a1359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('tmp')\n",
    "\n",
    "# This downloads two files: the config.json for the model, and the pytorch_model.bin to create the model.\n",
    "# pytorch_model.bin is the state dictionary - it contains all the model weights (your model's parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59392ef",
   "metadata": {},
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544182ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizers translate text into numerical data that can be processed by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa7ede9",
   "metadata": {},
   "source": [
    "## Word-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0320fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split on spaces, punctuation oor \n",
    "# Each word has a specific ID "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bf4c8e",
   "metadata": {},
   "source": [
    "### Disadvantages:\n",
    "* very similar words have entirely different meanings \n",
    "  (cat vs cats have two different embeddings so the model doesn't understand that these words are close)\n",
    "* the vocabulary size (total number of words) can end up very large\n",
    "* model can become very large / but we can limit the amount of words we add to the vocabulary (take 10K most frequent words for example)\n",
    "* however, out of vocab words result in a loss of information\n",
    "* to avoid these flaws, try character based tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3d19e7",
   "metadata": {},
   "source": [
    "## Character-based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e276188",
   "metadata": {},
   "source": [
    "* Helps to reduce the amount of unknown tokens \n",
    "* Vocabs are slimmer since you use a character-based vocabulary instead of a word-based vocab (so 256 vs 170K+ vocab size)\n",
    "* even words unseen during the tokenization training can still be tokenized\n",
    "* so out of vocab words will be fewer (can retain misspelled words instead of discarding them)\n",
    "\n",
    "### Disadvantages:\n",
    "* characters don't hold as much info as words\n",
    "* sequences are translated into very large amounts of tokens to be processed by model\n",
    "* it will reduce the size of input text allowed\n",
    "\n",
    "To get the best of both worlds, we can use a 3rd technique that combines the two approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2944e62",
   "metadata": {},
   "source": [
    "## Subword-based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9d60fa",
   "metadata": {},
   "source": [
    "Split the raw text into subwords.  \n",
    "'cats' is split into 'cat' and 's'\n",
    "* Frequently used words should not be split into smaller subwords\n",
    "* Rare words SHOULD be split into smaller subwords\n",
    "* the subwords provide a lot of semantic meaning\n",
    "* it can also identify the start of word tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d5be0a",
   "metadata": {},
   "source": [
    "# Loading and saving tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b12cb11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c4d2d0868849238966be947115060a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172749108a81430bbcd8fe49b1351803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17e2f0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abff2d5aab648d68d120b406d295e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Can also use AutoTokenizer to get the correct tokenizer class based on the checkpoint name\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b51337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86d7af0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tmp/tokenizer_config.json',\n",
       " 'tmp/special_tokens_map.json',\n",
       " 'tmp/vocab.txt',\n",
       " 'tmp/added_tokens.json',\n",
       " 'tmp/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aa4af5",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c6e7926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['let', \"'\", 's', 'go', 'jogging', 'in', 'the', 'park', 'for', 'exercise', 'to', 'token', '##ize', '!']\n",
      "[2292, 1005, 1055, 2175, 28233, 1999, 1996, 2380, 2005, 6912, 2000, 19204, 4697, 999]\n"
     ]
    }
   ],
   "source": [
    "# The tokenizer takes text as inputs and outputs numbers\n",
    "# raw text -> tokens -> special tokens -> input IDs\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# split the text into tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokens = tokenizer.tokenize(\"Let's go jogging in the park for exercise to tokenize!\")\n",
    "print(tokens)\n",
    "# different models have different symbols for start and end of words or other things\n",
    "\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2951acb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2292, 1005, 1055, 2175, 28233, 1999, 1996, 2380, 2005, 6912, 2000, 19204, 4697, 999, 102]\n",
      "[CLS] let's go jogging in the park for exercise to tokenize! [SEP]\n"
     ]
    }
   ],
   "source": [
    "# lastly the tokenizer adds special tokens that the model expects\n",
    "final_inputs = tokenizer.prepare_for_model(input_ids)\n",
    "print(final_inputs['input_ids'])\n",
    "\n",
    "# Use the decode method to see the final output \n",
    "inputs = tokenizer(\"Let's go jogging in the park for exercise to tokenize!\")\n",
    "print(tokenizer.decode(inputs['input_ids']))\n",
    "\n",
    "# CLS and SEP are used by this model, each model has different standards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b0f89c",
   "metadata": {},
   "source": [
    "# Handling Multiple Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0778d87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to batch inputs together?\n",
    "\n",
    "# Usually sentences have different lengths\n",
    "# BUT you can't build a tensor with different lengths so you need to pad the smaller sentence\n",
    "\n",
    "# the model has a specific padding ID that you need to use\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "tokenizer.pad_token_id\n",
    "\n",
    "# Padding and attention layers go hand in hand\n",
    "# The attention layers use the padding tokens in the context they look at for each token in the sentence.\n",
    "\n",
    "# We have to pass an attention mask! to tell the attention layers to ignore the padding.\n",
    "\n",
    "# use padding=True to tell the tokenizer to prepare the inputs with padding and the proper attention mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2c0c8c",
   "metadata": {},
   "source": [
    "Transformer models expect multiple sentences by default.  \n",
    "Batching is the act of sending multiple sentences through the model all at once.  \n",
    "<br>\n",
    "There is a limit to the lengths of sequences that Transformer models can handle.  \n",
    "Most can handle up to 512 or 1024 tokens max.  \n",
    "<br>\n",
    "Some models specialize in handling very long sequences (like Longformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8252ef",
   "metadata": {},
   "source": [
    "otherwise you should truncate your sequences by doing \n",
    "> sequence = sequence[:max_sequence_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4eecfe",
   "metadata": {},
   "source": [
    "# Wrap Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40658381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0787dfc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
       "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
