{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc74211",
   "metadata": {},
   "source": [
    "## Training a new tokenizer from an old one\n",
    "\n",
    "* You can train a tokenizer\n",
    "* This is not the same as training a model (which uses gradient descent).\n",
    "* Tokenizer training is a statistical process that tries to identify which subwords are the best to pick for a given corpus.\n",
    "* Tokenizer needs to be trained on a corpus that is similar to your task\n",
    "* Make sure it is a similar language, similar characters, similar domain (medical vs legal), and similar style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcf81967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81d75fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\n",
    "    \"huggingface-course/bert-base-uncased-tokenizer-without-normalizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ab08442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'is', 'a', 'sentence', 'adapted', 'to', 'our', 'token', '##izer']\n"
     ]
    }
   ],
   "source": [
    "text = 'here is a sentence adapted to our tokenizer'\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d63cd97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['af', '##ew', '##q', '##cin', '##q', '##wei', '##rn', '##v', '##q', '##3', '##ob', '##ni', '##3', '##p', '##0', '##p', '##3', 'q', '##vr', '##m', '##n', '##q', '##er', '##bn', '##q', '##3', '##r', '##0', '##9', '##q', 'q', '##v', '##wn', '##0', '##q', '##w', '##3', '##nr', '##b', '##q', '##0', '##3', '##r', '##q']\n"
     ]
    }
   ],
   "source": [
    "text2 = 'afewqcinqweirnvq3obni3p0p3 qvrmnqerbnq3r09q qvwn0qw3nrbq03rq'\n",
    "print(tokenizer.tokenize(text2))\n",
    "\n",
    "# unknown tokens are a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80fa32d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'medical', 'vocabulary', 'is', 'divided', 'into', 'many', 'sub', '-', 'token', '##s', ':', 'para', '##ce', '##tam', '##ol', 'ph', '##ray', '##ng', '##itis']\n"
     ]
    }
   ],
   "source": [
    "text3 = 'the medical vocabulary is divided into many sub-tokens: paracetamol phrayngitis'\n",
    "print(tokenizer.tokenize(text3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2650e4e5",
   "metadata": {},
   "source": [
    "## Steps to train a new tokenizer\n",
    "* Gather a corpus of texts\n",
    "* Choose a tokenizer architecture\n",
    "* Train the tokenizer on the corpus\n",
    "* Save the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99fd3de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use \"AutoTokenizer.train_new_from_iterator\" method to train a tokenizer using a known architecture on a new corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23a16699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmarlowe/opt/anaconda3/envs/transformers_course/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec0e18b16a347d4b401b09bcbeb34aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2055b9e011ca4a5d81a3c81ea8af2303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/19.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset code_search_net/python (download: 897.32 MiB, generated: 1.62 GiB, post-processed: Unknown size, total: 2.49 GiB) to /Users/jmarlowe/.cache/huggingface/datasets/code_search_net/python/1.0.0/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26243ee1e64141898be23cd7b831a698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1979656f79049a29243513bf3ad223c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/941M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895f470410a84596865f5fa452250971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da371feb81d4283aa9cdae4d5c92330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/412178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/22176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/23107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset code_search_net downloaded and prepared to /Users/jmarlowe/.cache/huggingface/datasets/code_search_net/python/1.0.0/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443fd4d742c7464a86736163a3ec11b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gather your training corpus\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset('code_search_net', 'python')\n",
    "\n",
    "def get_training_corpus():\n",
    "    dataset = raw_datasets['train']\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx: start_idx + 1000]\n",
    "        yield samples['whole_func_string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bbd66ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('code-search-net-tokenizer/tokenizer_config.json',\n",
       " 'code-search-net-tokenizer/special_tokens_map.json',\n",
       " 'code-search-net-tokenizer/vocab.json',\n",
       " 'code-search-net-tokenizer/merges.txt',\n",
       " 'code-search-net-tokenizer/added_tokens.json',\n",
       " 'code-search-net-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "training_corpus = get_training_corpus()\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "new_tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)\n",
    "\n",
    "new_tokenizer.save_pretrained('code-search-net-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff3f0cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "my_list = [i for i in range(10)]\n",
    "\n",
    "print(type(my_list))\n",
    "print(my_list)\n",
    "print(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9836e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "my_generator = (i for i in range(10))\n",
    "\n",
    "print(type(my_generator))\n",
    "print(list(my_generator))\n",
    "print(list(my_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e95fcfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note AutoTokenizer.train_new_from_iterator()  only works if you are using a \"fast\" tokenizer. \n",
    "# The Transformers library contains 2 types of tokenizers: some in Python (slow) and some in Rust (fast)\n",
    "\n",
    "# Python is the language used most often for Data Science and Deep Learning but it is super slow to do parallelized stuff in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "749f2f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most of the Transformer models have a \"fast\" tokenizer available. \n",
    "# The AutoTokenizer API always selects the fast tokenizer for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a234632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast tokenizer - need to use batched=True to get the real performance benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d98cf8",
   "metadata": {},
   "source": [
    "### Normalization and pre-tokenization\n",
    "Before splitting a text into subtokens, the tokenizer performs 1- normalization, and 2- pre-tokenization.  \n",
    "Then come steps 3- model, and 4- postprocessing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "891e52a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tokenizers.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(type(tokenizer.backend_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3e1a536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a text with hello how are u? and capital letters too hey?\n"
     ]
    }
   ],
   "source": [
    "text = 'This is a text with Héllò hôw are ü? and CAPITAL LETTERS too Hey?'\n",
    "\n",
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5232f596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('how', (7, 10)),\n",
       " ('are', (11, 14)),\n",
       " ('you', (16, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce981846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_piece = tokenization algorithm for preprocessing text \n",
    "# it replaces spaces with \"_\" character\n",
    "# it has reversible tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf1703",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding tokenization\n",
    "* initially developed as an algorithm to compress texts\n",
    "* then used by OpenAI for tokenization when pretraining GPT\n",
    "* used by many Transformer models including GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf2c3538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the example you are tokenizing contains a character that is not in the training corpus, that character will be \n",
    "# converted to the unknown token. Be careful when handling emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a45469fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"merges\" rules to combine two elements (letters or parts of word) of the existing vocab together into a new one\n",
    "# start with two word tokens which then get combined into longer subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa8b65b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization, pre-tokenization, split words into individual characters, and\n",
    "# applying the merge rules learned in order on those splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2e8e95",
   "metadata": {},
   "source": [
    "## WordPiece tokenization\n",
    "* Google used this to pretrain BERT\n",
    "* Google never open-sourced the implementation\n",
    "* similar to BPE but actual tokenization is done differently\n",
    "* it only saves the final vocabulary, not the merge rules learned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b173834c",
   "metadata": {},
   "source": [
    "## Unigram tokenization\n",
    "* Used by T5\n",
    "* Unigram model is a type of statistical language model - assumes that the occurrence of each word is independent of its previous word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a664a",
   "metadata": {},
   "source": [
    "# Tokenizer - building from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3dd9a",
   "metadata": {},
   "source": [
    "* Normalizers\n",
    "* PreTokenizers\n",
    "* Models\n",
    "* Trainers\n",
    "* PostProcessors\n",
    "* Decoders\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ddc6d7",
   "metadata": {},
   "source": [
    "1. Create a training dataset\n",
    "2. Create a backend_tokenizer with HF tokenizers\n",
    "3. Load the backend_tokenizer in a HF transformers tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abecae24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmarlowe/opt/anaconda3/envs/transformers_course/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a53146107eba473097c5d24d2ba5bddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ce77a7bfaf4cfc8ee9133c5cafdc71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/6.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wikitext/wikitext-2-raw-v1 (download: 4.50 MiB, generated: 12.90 MiB, post-processed: Unknown size, total: 17.40 MiB) to /Users/jmarlowe/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a7bc616b524cff8dfc9e919ebf6fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.72M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wikitext downloaded and prepared to /Users/jmarlowe/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "372cc489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01fb6a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wikitext-2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(len(dataset)):\n",
    "        f.write(dataset[i][\"text\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b8b65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac1c18d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c501e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c5fc245",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da0da9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cc0d7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "caaed317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.model = models.WordPiece(unk_token=\"[UNK]\")\n",
    "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222cbd0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
